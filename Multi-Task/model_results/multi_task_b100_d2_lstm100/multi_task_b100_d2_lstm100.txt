______________________________________________________________________________________________________________________________________________________
Layer (type)                                     Output Shape                     Param #           Connected to                                      
======================================================================================================================================================
input_16 (InputLayer)                            (None, 73, 54)                   0                                                                   
______________________________________________________________________________________________________________________________________________________
char_embedding (TimeDistributed)                 (None, 73, 54, 100)              16500             input_16[0][0]                                    
______________________________________________________________________________________________________________________________________________________
dropout_22 (Dropout)                             (None, 73, 54, 100)              0                 char_embedding[0][0]                              
______________________________________________________________________________________________________________________________________________________
input_15 (InputLayer)                            (None, 73)                       0                                                                   
______________________________________________________________________________________________________________________________________________________
time_distributed_15 (TimeDistributed)            (None, 73, 54, 50)               25200             dropout_22[0][0]                                  
______________________________________________________________________________________________________________________________________________________
embedding_15 (Embedding)                         (None, 73, 300)                  17130300          input_15[0][0]                                    
______________________________________________________________________________________________________________________________________________________
time_distributed_16 (TimeDistributed)            (None, 73, 2700)                 0                 time_distributed_15[0][0]                         
______________________________________________________________________________________________________________________________________________________
concatenate_8 (Concatenate)                      (None, 73, 3000)                 0                 embedding_15[0][0]                                
                                                                                                    time_distributed_16[0][0]                         
______________________________________________________________________________________________________________________________________________________
dropout_23 (Dropout)                             (None, 73, 3000)                 0                 concatenate_8[0][0]                               
______________________________________________________________________________________________________________________________________________________
bidirectional_16 (Bidirectional)                 (None, 73, 200)                  2480800           dropout_23[0][0]                                  
______________________________________________________________________________________________________________________________________________________
dropout_24 (Dropout)                             (None, 73, 200)                  0                 bidirectional_16[0][0]                            
______________________________________________________________________________________________________________________________________________________
dense_22 (Dense)                                 (None, 73, 28)                   5628              dropout_24[0][0]                                  
______________________________________________________________________________________________________________________________________________________
dense_23 (Dense)                                 (None, 73, 11)                   2211              dropout_24[0][0]                                  
______________________________________________________________________________________________________________________________________________________
dense_24 (Dense)                                 (None, 73, 5)                    1005              dropout_24[0][0]                                  
______________________________________________________________________________________________________________________________________________________
crf_22 (CRF)                                     (None, 73, 28)                   1652              dense_22[0][0]                                    
______________________________________________________________________________________________________________________________________________________
crf_23 (CRF)                                     (None, 73, 11)                   275               dense_23[0][0]                                    
______________________________________________________________________________________________________________________________________________________
crf_24 (CRF)                                     (None, 73, 5)                    65                dense_24[0][0]                                    
======================================================================================================================================================
Total params: 19,663,636
Trainable params: 19,663,636
Non-trainable params: 0
______________________________________________________________________________________________________________________________________________________
None 




Train on 49616 samples, validate on 2258 samples
Epoch 1/25
 - 1465s - loss: 0.5502 - crf_22_loss: 0.2487 - crf_23_loss: 0.2089 - crf_24_loss: 0.0926 - crf_22_acc_1: 0.9254 - crf_22_acc_2: 0.7708 - crf_22_acc_3: 0.7739 - crf_23_acc_1: 0.7705 - crf_23_acc_2: 0.9162 - crf_23_acc_3: 0.7726 - crf_24_acc_1: 0.7725 - crf_24_acc_2: 0.7708 - crf_24_acc_3: 0.9633 - val_loss: 0.1850 - val_crf_22_loss: 0.0917 - val_crf_23_loss: 0.0833 - val_crf_24_loss: 0.0100 - val_crf_22_acc_1: 0.9671 - val_crf_22_acc_2: 0.8129 - val_crf_22_acc_3: 0.8135 - val_crf_23_acc_1: 0.8134 - val_crf_23_acc_2: 0.9549 - val_crf_23_acc_3: 0.8157 - val_crf_24_acc_1: 0.8133 - val_crf_24_acc_2: 0.8145 - val_crf_24_acc_3: 0.9909 - val_f1: 0.8392 - val_crf_22_f1: 0.8141 - val_crf_23_f1: 0.7563 - val_crf_24_f1: 0.9471
Epoch 2/25
 - 1448s - loss: 0.1824 - crf_22_loss: 0.0908 - crf_23_loss: 0.0732 - crf_24_loss: 0.0184 - crf_22_acc_1: 0.9606 - crf_22_acc_2: 0.7723 - crf_22_acc_3: 0.7756 - crf_23_acc_1: 0.7722 - crf_23_acc_2: 0.9513 - crf_23_acc_3: 0.7737 - crf_24_acc_1: 0.7751 - crf_24_acc_2: 0.7731 - crf_24_acc_3: 0.9834 - val_loss: 0.0941 - val_crf_22_loss: 0.0529 - val_crf_23_loss: 0.0406 - val_crf_24_loss: 6.0038e-04 - val_crf_22_acc_1: 0.9721 - val_crf_22_acc_2: 0.8130 - val_crf_22_acc_3: 0.8134 - val_crf_23_acc_1: 0.8134 - val_crf_23_acc_2: 0.9604 - val_crf_23_acc_3: 0.8150 - val_crf_24_acc_1: 0.8135 - val_crf_24_acc_2: 0.8157 - val_crf_24_acc_3: 0.9888 - val_f1: 0.8670 - val_crf_22_f1: 0.8568 - val_crf_23_f1: 0.7967 - val_crf_24_f1: 0.9476
Epoch 3/25
 - 1451s - loss: 0.0883 - crf_22_loss: 0.0543 - crf_23_loss: 0.0354 - crf_24_loss: -1.3408e-03 - crf_22_acc_1: 0.9663 - crf_22_acc_2: 0.7723 - crf_22_acc_3: 0.7755 - crf_23_acc_1: 0.7722 - crf_23_acc_2: 0.9582 - crf_23_acc_3: 0.7736 - crf_24_acc_1: 0.7752 - crf_24_acc_2: 0.7731 - crf_24_acc_3: 0.9859 - val_loss: 0.0344 - val_crf_22_loss: 0.0294 - val_crf_23_loss: 0.0171 - val_crf_24_loss: -1.2166e-02 - val_crf_22_acc_1: 0.9773 - val_crf_22_acc_2: 0.8130 - val_crf_22_acc_3: 0.8134 - val_crf_23_acc_1: 0.8134 - val_crf_23_acc_2: 0.9630 - val_crf_23_acc_3: 0.8152 - val_crf_24_acc_1: 0.8132 - val_crf_24_acc_2: 0.8154 - val_crf_24_acc_3: 0.9896 - val_f1: 0.8769 - val_crf_22_f1: 0.8794 - val_crf_23_f1: 0.8054 - val_crf_24_f1: 0.9461
Epoch 4/25
 - 1450s - loss: 0.0325 - crf_22_loss: 0.0328 - crf_23_loss: 0.0139 - crf_24_loss: -1.4142e-02 - crf_22_acc_1: 0.9692 - crf_22_acc_2: 0.7722 - crf_22_acc_3: 0.7754 - crf_23_acc_1: 0.7721 - crf_23_acc_2: 0.9624 - crf_23_acc_3: 0.7735 - crf_24_acc_1: 0.7752 - crf_24_acc_2: 0.7731 - crf_24_acc_3: 0.9872 - val_loss: -3.0730e-03 - val_crf_22_loss: 0.0176 - val_crf_23_loss: 0.0019 - val_crf_24_loss: -2.2503e-02 - val_crf_22_acc_1: 0.9756 - val_crf_22_acc_2: 0.8130 - val_crf_22_acc_3: 0.8134 - val_crf_23_acc_1: 0.8134 - val_crf_23_acc_2: 0.9652 - val_crf_23_acc_3: 0.8153 - val_crf_24_acc_1: 0.8134 - val_crf_24_acc_2: 0.8150 - val_crf_24_acc_3: 0.9912 - val_f1: 0.8793 - val_crf_22_f1: 0.8741 - val_crf_23_f1: 0.8121 - val_crf_24_f1: 0.9518
Epoch 5/25
 - 1451s - loss: -7.1402e-03 - crf_22_loss: 0.0175 - crf_23_loss: -6.2540e-04 - crf_24_loss: -2.4044e-02 - crf_22_acc_1: 0.9712 - crf_22_acc_2: 0.7722 - crf_22_acc_3: 0.7754 - crf_23_acc_1: 0.7721 - crf_23_acc_2: 0.9657 - crf_23_acc_3: 0.7734 - crf_24_acc_1: 0.7753 - crf_24_acc_2: 0.7731 - crf_24_acc_3: 0.9878 - val_loss: -2.6246e-02 - val_crf_22_loss: 0.0102 - val_crf_23_loss: -7.6855e-03 - val_crf_24_loss: -2.8715e-02 - val_crf_22_acc_1: 0.9734 - val_crf_22_acc_2: 0.8130 - val_crf_22_acc_3: 0.8135 - val_crf_23_acc_1: 0.8134 - val_crf_23_acc_2: 0.9647 - val_crf_23_acc_3: 0.8157 - val_crf_24_acc_1: 0.8134 - val_crf_24_acc_2: 0.8148 - val_crf_24_acc_3: 0.9903 - val_f1: 0.8699 - val_crf_22_f1: 0.8551 - val_crf_23_f1: 0.8090 - val_crf_24_f1: 0.9455
Epoch 6/25
 - 1447s - loss: -3.8792e-02 - crf_22_loss: 0.0055 - crf_23_loss: -1.1796e-02 - crf_24_loss: -3.2536e-02 - crf_22_acc_1: 0.9730 - crf_22_acc_2: 0.7722 - crf_22_acc_3: 0.7754 - crf_23_acc_1: 0.7721 - crf_23_acc_2: 0.9682 - crf_23_acc_3: 0.7734 - crf_24_acc_1: 0.7752 - crf_24_acc_2: 0.7731 - crf_24_acc_3: 0.9881 - val_loss: -5.1682e-02 - val_crf_22_loss: -9.3726e-04 - val_crf_23_loss: -1.4500e-02 - val_crf_24_loss: -3.6244e-02 - val_crf_22_acc_1: 0.9772 - val_crf_22_acc_2: 0.8130 - val_crf_22_acc_3: 0.8135 - val_crf_23_acc_1: 0.8134 - val_crf_23_acc_2: 0.9592 - val_crf_23_acc_3: 0.8153 - val_crf_24_acc_1: 0.8136 - val_crf_24_acc_2: 0.8150 - val_crf_24_acc_3: 0.9893 - val_f1: 0.8698 - val_crf_22_f1: 0.8801 - val_crf_23_f1: 0.7878 - val_crf_24_f1: 0.9416
Epoch 7/25
 - 1446s - loss: -6.6787e-02 - crf_22_loss: -4.8695e-03 - crf_23_loss: -2.1340e-02 - crf_24_loss: -4.0578e-02 - crf_22_acc_1: 0.9743 - crf_22_acc_2: 0.7722 - crf_22_acc_3: 0.7753 - crf_23_acc_1: 0.7721 - crf_23_acc_2: 0.9703 - crf_23_acc_3: 0.7734 - crf_24_acc_1: 0.7753 - crf_24_acc_2: 0.7730 - crf_24_acc_3: 0.9886 - val_loss: -6.8869e-02 - val_crf_22_loss: -6.0878e-03 - val_crf_23_loss: -2.1799e-02 - val_crf_24_loss: -4.0982e-02 - val_crf_22_acc_1: 0.9748 - val_crf_22_acc_2: 0.8131 - val_crf_22_acc_3: 0.8134 - val_crf_23_acc_1: 0.8134 - val_crf_23_acc_2: 0.9626 - val_crf_23_acc_3: 0.8153 - val_crf_24_acc_1: 0.8134 - val_crf_24_acc_2: 0.8153 - val_crf_24_acc_3: 0.9849 - val_f1: 0.8695 - val_crf_22_f1: 0.8737 - val_crf_23_f1: 0.8083 - val_crf_24_f1: 0.9266
Epoch 8/25
 - 1446s - loss: -9.2486e-02 - crf_22_loss: -1.4303e-02 - crf_23_loss: -3.0018e-02 - crf_24_loss: -4.8164e-02 - crf_22_acc_1: 0.9757 - crf_22_acc_2: 0.7721 - crf_22_acc_3: 0.7753 - crf_23_acc_1: 0.7721 - crf_23_acc_2: 0.9726 - crf_23_acc_3: 0.7733 - crf_24_acc_1: 0.7753 - crf_24_acc_2: 0.7731 - crf_24_acc_3: 0.9889 - val_loss: -9.2819e-02 - val_crf_22_loss: -1.5463e-02 - val_crf_23_loss: -2.9216e-02 - val_crf_24_loss: -4.8140e-02 - val_crf_22_acc_1: 0.9768 - val_crf_22_acc_2: 0.8130 - val_crf_22_acc_3: 0.8136 - val_crf_23_acc_1: 0.8133 - val_crf_23_acc_2: 0.9593 - val_crf_23_acc_3: 0.8153 - val_crf_24_acc_1: 0.8134 - val_crf_24_acc_2: 0.8154 - val_crf_24_acc_3: 0.9877 - val_f1: 0.8653 - val_crf_22_f1: 0.8785 - val_crf_23_f1: 0.7860 - val_crf_24_f1: 0.9314
Epoch 9/25
 - 1466s - loss: -1.1696e-01 - crf_22_loss: -2.3082e-02 - crf_23_loss: -3.8207e-02 - crf_24_loss: -5.5670e-02 - crf_22_acc_1: 0.9768 - crf_22_acc_2: 0.7721 - crf_22_acc_3: 0.7753 - crf_23_acc_1: 0.7721 - crf_23_acc_2: 0.9741 - crf_23_acc_3: 0.7733 - crf_24_acc_1: 0.7753 - crf_24_acc_2: 0.7731 - crf_24_acc_3: 0.9891 - val_loss: -1.0590e-01 - val_crf_22_loss: -1.9241e-02 - val_crf_23_loss: -3.2854e-02 - val_crf_24_loss: -5.3801e-02 - val_crf_22_acc_1: 0.9696 - val_crf_22_acc_2: 0.8131 - val_crf_22_acc_3: 0.8135 - val_crf_23_acc_1: 0.8133 - val_crf_23_acc_2: 0.9506 - val_crf_23_acc_3: 0.8151 - val_crf_24_acc_1: 0.8135 - val_crf_24_acc_2: 0.8152 - val_crf_24_acc_3: 0.9827 - val_f1: 0.8353 - val_crf_22_f1: 0.8542 - val_crf_23_f1: 0.7341 - val_crf_24_f1: 0.9176

-------------------------------------------
Best F1 score: 0.879316349248   (epoch number 4)
For task 1
====================================================================================
With padding into account
                           precision    recall  f1-score   support

             abbreviation     0.9762    0.5541    0.7069       148
        archivalreference     0.8797    0.6518    0.7488       853
              archive_lib     0.8509    0.9417    0.8940       103
               attachment     0.0000    0.0000    0.0000         0
                   author     0.9333    0.9394    0.9363      4948
                      box     0.6825    0.3909    0.4971       110
              cartulation     0.5333    1.0000    0.6957         8
              conjunction     0.5312    0.7612    0.6258       134
                     date     0.3652    0.7241    0.4855        58
                    filza     1.0000    1.0000    1.0000         3
                   folder     0.2857    0.6429    0.3956        28
                foliation     0.6000    1.0000    0.7500        12
                     null     1.0000    1.0000    1.0000    133958
             numbered_ref     0.1007    0.2500    0.1435       120
                        o     0.2737    0.3537    0.3086       752
               pagination     0.9280    0.9614    0.9444      1139
   publicationnumber-year     0.6523    0.8220    0.7274       646
         publicationplace     0.8798    0.9334    0.9058      1592
publicationspecifications     0.3397    0.3817    0.3595       372
                publisher     0.8430    0.8593    0.8511      1443
                      ref     0.1176    0.6667    0.2000         3
                 registry     1.0000    0.1039    0.1882       154
                   series     0.9406    0.6209    0.7480       153
                    title     0.9479    0.9254    0.9365     15560
                     tomo     0.8222    0.3304    0.4713       224
                   volume     0.4423    0.6498    0.5263       277
                     year     0.9066    0.8247    0.8637      2036

              avg / total     0.9791    0.9756    0.9764    164834


----------------------------------------------

Without the padding:
                           precision    recall  f1-score   support

             abbreviation     0.9762    0.5541    0.7069       148
        archivalreference     0.8797    0.6518    0.7488       853
              archive_lib     0.8509    0.9417    0.8940       103
               attachment     0.0000    0.0000    0.0000         0
                   author     0.9333    0.9394    0.9363      4948
                      box     0.6825    0.3909    0.4971       110
              cartulation     0.5333    1.0000    0.6957         8
                   column     0.0000    0.0000    0.0000         0
              conjunction     0.5312    0.7612    0.6258       134
                     date     0.3652    0.7241    0.4855        58
                    filza     1.0000    1.0000    1.0000         3
                   folder     0.2857    0.6429    0.3956        28
                foliation     0.6000    1.0000    0.7500        12
             numbered_ref     0.1007    0.2500    0.1435       120
                        o     0.2737    0.3537    0.3086       752
               pagination     0.9280    0.9614    0.9444      1139
   publicationnumber-year     0.6523    0.8220    0.7274       646
         publicationplace     0.8798    0.9334    0.9058      1592
publicationspecifications     0.3397    0.3817    0.3595       372
                publisher     0.8430    0.8593    0.8511      1443
                      ref     0.1176    0.6667    0.2000         3
                 registry     1.0000    0.1039    0.1882       154
                   series     0.9406    0.6209    0.7480       153
                    title     0.9479    0.9254    0.9365     15560
                     tomo     0.8222    0.3304    0.4713       224
                   volume     0.4423    0.6498    0.5263       277
                     year     0.9066    0.8247    0.8637      2036

              avg / total     0.8884    0.8695    0.8741     30876



For task 2
====================================================================================
With padding into account
                   precision    recall  f1-score   support

b-meta-annotation     0.7791    0.5776    0.6634       348
        b-primary     0.6667    0.1714    0.2727       105
      b-secondary     0.7454    0.7077    0.7261       852
e-meta-annotation     0.8985    0.7172    0.7977      1061
        e-primary     0.6422    0.4667    0.5405       300
      e-secondary     0.7824    0.8690    0.8234      1717
i-meta-annotation     0.8475    0.7486    0.7950      9981
        i-primary     0.8556    0.7101    0.7761      1728
      i-secondary     0.8169    0.9088    0.8604     14357
             null     1.0000    1.0000    1.0000    133958
                o     0.2925    0.4028    0.3389       427

      avg / total     0.9659    0.9652    0.9648    164834


----------------------------------------------

Without the padding:
                   precision    recall  f1-score   support

b-meta-annotation     0.7791    0.5776    0.6634       348
        b-primary     0.6667    0.1714    0.2727       105
      b-secondary     0.7454    0.7077    0.7261       852
e-meta-annotation     0.8985    0.7172    0.7977      1061
        e-primary     0.6422    0.4667    0.5405       300
      e-secondary     0.7824    0.8690    0.8234      1717
i-meta-annotation     0.8475    0.7486    0.7950      9981
        i-primary     0.8556    0.7101    0.7761      1728
      i-secondary     0.8169    0.9088    0.8604     14357
                o     0.2925    0.4028    0.3389       427

      avg / total     0.8180    0.8140    0.8121     30876



For task 3
====================================================================================
With padding into account
             precision    recall  f1-score   support

        b-r     0.8487    0.7004    0.7674      1305
        e-r     0.9664    0.6588    0.7835      1310
        i-r     0.9671    0.9871    0.9770     27834
       null     1.0000    1.0000    1.0000    133958
          o     0.3589    0.4169    0.3857       427

avg / total     0.9913    0.9912    0.9910    164834


----------------------------------------------

Without the padding:
             precision    recall  f1-score   support

        b-r     0.8487    0.7004    0.7674      1305
        e-r     0.9664    0.6588    0.7835      1310
        i-r     0.9671    0.9871    0.9770     27834
          o     0.3589    0.4169    0.3857       427

avg / total     0.9537    0.9532    0.9518     30876




Confusion matrix, without normalization
[[    36      0     96     16      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     8      5    831      9      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [    13      0     79     11      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [   872      2   4066      8      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     0      3    107      0      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     0      2      6      0      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     0      0    132      2      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     0      8     50      0      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     0      1      2      0      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     1      1     26      0      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     0      0     12      0      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     0      0      0      0      0      0      0      0      0      0
       0      0 133958      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     1      5    110      4      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     6     10    546    190      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     0    184    931     24      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     1     21    624      0      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     1      3   1587      1      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     1      3    344     24      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     2      1   1440      0      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     0      0      3      0      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     3      5    146      0      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     0      0    153      0      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [   112     28  15365     55      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     0      7    217      0      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [     0      1    275      1      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]
 [    20    603   1262    151      0      0      0      0      0      0
       0      0      0      0      0      0      0      0      0      0
       0      0      0      0      0      0      0]]
Normalized confusion matrix
[[  2.43243243e-01   0.00000000e+00   6.48648649e-01   1.08108108e-01
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  9.37866354e-03   5.86166471e-03   9.74208675e-01   1.05509965e-02
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  1.26213592e-01   0.00000000e+00   7.66990291e-01   1.06796117e-01
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [             nan              nan              nan              nan
               nan              nan              nan              nan
               nan              nan              nan              nan
               nan              nan              nan              nan
               nan              nan              nan              nan
               nan              nan              nan              nan
               nan              nan              nan]
 [  1.76232821e-01   4.04203719e-04   8.21746160e-01   1.61681487e-03
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   2.72727273e-02   9.72727273e-01   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   2.50000000e-01   7.50000000e-01   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   0.00000000e+00   9.85074627e-01   1.49253731e-02
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.37931034e-01   8.62068966e-01   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   3.33333333e-01   6.66666667e-01   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  3.57142857e-02   3.57142857e-02   9.28571429e-01   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  8.33333333e-03   4.16666667e-02   9.16666667e-01   3.33333333e-02
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  7.97872340e-03   1.32978723e-02   7.26063830e-01   2.52659574e-01
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.61545215e-01   8.17383670e-01   2.10711150e-02
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  1.54798762e-03   3.25077399e-02   9.65944272e-01   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  6.28140704e-04   1.88442211e-03   9.96859296e-01   6.28140704e-04
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  2.68817204e-03   8.06451613e-03   9.24731183e-01   6.45161290e-02
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  1.38600139e-03   6.93000693e-04   9.97920998e-01   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  1.94805195e-02   3.24675325e-02   9.48051948e-01   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  7.19794344e-03   1.79948586e-03   9.87467866e-01   3.53470437e-03
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   3.12500000e-02   9.68750000e-01   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   3.61010830e-03   9.92779783e-01   3.61010830e-03
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  9.82318271e-03   2.96168959e-01   6.19842829e-01   7.41650295e-02
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]]
Confusion matrix, without normalization
[[   269      0     42     37      0      0      0      0      0      0
       0]
 [    20      0     84      1      0      0      0      0      0      0
       0]
 [   625      0    135     92      0      0      0      0      0      0
       0]
 [     0    253    788     20      0      0      0      0      0      0
       0]
 [     1     42    251      6      0      0      0      0      0      0
       0]
 [     1    568   1138     10      0      0      0      0      0      0
       0]
 [    46      4   9868     63      0      0      0      0      0      0
       0]
 [     4      8   1693     23      0      0      0      0      0      0
       0]
 [   106     13  14172     66      0      0      0      0      0      0
       0]
 [     0      0      0      0      0      0      0      0      0 133958
       0]
 [     5      5    239    178      0      0      0      0      0      0
       0]]
Normalized confusion matrix
[[  7.72988506e-01   0.00000000e+00   1.20689655e-01   1.06321839e-01
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  1.90476190e-01   0.00000000e+00   8.00000000e-01   9.52380952e-03
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  7.33568075e-01   0.00000000e+00   1.58450704e-01   1.07981221e-01
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   2.38454288e-01   7.42695570e-01   1.88501414e-02
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  3.33333333e-03   1.40000000e-01   8.36666667e-01   2.00000000e-02
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  5.82411182e-04   3.30809552e-01   6.62783925e-01   5.82411182e-03
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  4.60875664e-03   4.00761447e-04   9.88678489e-01   6.31199279e-03
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  2.31481481e-03   4.62962963e-03   9.79745370e-01   1.33101852e-02
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  7.38315804e-03   9.05481647e-04   9.87114300e-01   4.59706067e-03
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   1.00000000e+00   0.00000000e+00]
 [  1.17096019e-02   1.17096019e-02   5.59718970e-01   4.16861827e-01
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00]]
Confusion matrix, without normalization
[[   914      0    261      0    130]
 [     0    863    434      0     13]
 [   158     25  27476      0    175]
 [     0      0      0 133958      0]
 [     5      5    239      0    178]]
Normalized confusion matrix
[[  7.00383142e-01   0.00000000e+00   2.00000000e-01   0.00000000e+00
    9.96168582e-02]
 [  0.00000000e+00   6.58778626e-01   3.31297710e-01   0.00000000e+00
    9.92366412e-03]
 [  5.67651074e-03   8.98182079e-04   9.87138033e-01   0.00000000e+00
    6.28727456e-03]
 [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00
    0.00000000e+00]
 [  1.17096019e-02   1.17096019e-02   5.59718970e-01   0.00000000e+00
    4.16861827e-01]]
